{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-10T04:34:36.394330Z",
     "start_time": "2025-04-10T04:34:32.902396Z"
    }
   },
   "source": [
    "import torch\n",
    "from config import config\n",
    "from model import Transformer\n",
    "from data_utils import build_vocab, token_transform, create_src_mask, tensor_transform\n",
    "from torchtext.datasets import Multi30k\n",
    "import sacrebleu\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import shutil\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T04:34:39.611281Z",
     "start_time": "2025-04-10T04:34:36.400652Z"
    }
   },
   "cell_type": "code",
   "source": "vocab_transform = build_vocab()",
   "id": "f4a5d06c2152284a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T04:34:39.836155Z",
     "start_time": "2025-04-10T04:34:39.833794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config[\"src_vocab_size\"] = len(vocab_transform['de'])\n",
    "config[\"tgt_vocab_size\"] = len(vocab_transform['en'])"
   ],
   "id": "48bf31f197b2f772",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T04:34:40.371065Z",
     "start_time": "2025-04-10T04:34:39.849890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Transformer(\n",
    "    src_vocab_size=config[\"src_vocab_size\"],\n",
    "    tgt_vocab_size=config[\"tgt_vocab_size\"],\n",
    "    model_dim=config[\"model_dim\"],\n",
    "    num_heads=config[\"num_heads\"],\n",
    "    ff_dim=config[\"ff_dim\"],\n",
    "    num_layers=config[\"num_layers\"],\n",
    "    max_seq_length=config[\"max_seq_length\"],\n",
    "    dropout=config[\"dropout\"]\n",
    ")"
   ],
   "id": "213d15f0a5247e92",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T04:34:40.705524Z",
     "start_time": "2025-04-10T04:34:40.379626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load(\"checkpoints/transformer_best.pt\"))\n",
    "model.eval()"
   ],
   "id": "c3141cc76f43d8b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (src_embedding): InputEmbedding(\n",
       "    (embedding): Embedding(18669, 512)\n",
       "  )\n",
       "  (tgt_embedding): InputEmbedding(\n",
       "    (embedding): Embedding(9795, 512)\n",
       "  )\n",
       "  (src_positional_encoding): PositionalEncoding()\n",
       "  (tgt_positional_encoding): PositionalEncoding()\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (Wo): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): FeedForward(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection_layer): Linear(in_features=512, out_features=9795, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T04:34:40.725674Z",
     "start_time": "2025-04-10T04:34:40.718733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, bos_idx, eos_idx, config):\n",
    "    \"\"\"\n",
    "    Generate a translation using greedy decoding.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        src: (1, src_seq_len) input tensor (already tokenized and indexed)\n",
    "        src_mask: (1, 1, 1, src_seq_len) mask for source\n",
    "        max_len: maximum length of the generated sentence\n",
    "        bos_idx: index of <bos> token in target vocab\n",
    "        eos_idx: index of <eos> token in target vocab\n",
    "        config: config dict for accessing padding and max_seq_length\n",
    "\n",
    "    Returns:\n",
    "        output: (1, generated_seq_len) tensor of predicted token IDs\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Step 1: Encode source\n",
    "    with torch.no_grad():\n",
    "        src_emb = model.src_embedding(src)\n",
    "        src_emb = model.src_positional_encoding(src_emb)\n",
    "        memory = model.encoder(src_emb, src_mask)\n",
    "\n",
    "    # Step 2: Start decoding with BOS token\n",
    "    ys = torch.ones((1, 1), dtype=torch.long).fill_(bos_idx).to(src.device)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        tgt_emb = model.tgt_embedding(ys)\n",
    "        tgt_emb = model.tgt_positional_encoding(tgt_emb)\n",
    "\n",
    "        tgt_mask = (ys != config[\"pad_idx\"]).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_sub_mask = torch.tril(torch.ones((ys.size(1), ys.size(1)), device=ys.device)).bool()\n",
    "        combined_mask = tgt_mask & tgt_sub_mask.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        # Decode\n",
    "        with torch.no_grad():\n",
    "            out = model.decoder(tgt_emb, memory, src_mask, combined_mask)\n",
    "            logits = model.projection_layer(out[:, -1])  # (1, vocab_size)\n",
    "            next_token = torch.argmax(logits, dim=-1).unsqueeze(1)  # (1, 1)\n",
    "\n",
    "        ys = torch.cat([ys, next_token], dim=1)\n",
    "\n",
    "        # Stop if EOS is generated\n",
    "        if next_token.item() == eos_idx:\n",
    "            break\n",
    "\n",
    "    return ys\n"
   ],
   "id": "b9879724c21f2d3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T04:34:40.760730Z",
     "start_time": "2025-04-10T04:34:40.736563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_iter = list(Multi30k(split='valid'))\n",
    "de_sentence, en_sentence = test_iter[0]\n",
    "\n",
    "src_text = de_sentence.lower()\n",
    "src_tokens = token_transform['de'](src_text)\n",
    "src_ids = [vocab_transform['de'][tok] for tok in src_tokens]\n",
    "src_tensor = tensor_transform(src_ids).unsqueeze(0)\n",
    "src_mask = create_src_mask(src_tensor, config[\"pad_idx\"])"
   ],
   "id": "5b23c44e89e4d732",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T04:34:41.080810Z",
     "start_time": "2025-04-10T04:34:40.773988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = greedy_decode(\n",
    "        model,\n",
    "        src_tensor,\n",
    "        src_mask,\n",
    "        max_len=config[\"max_seq_length\"],\n",
    "        bos_idx=config[\"bos_idx\"],\n",
    "        eos_idx=config[\"eos_idx\"],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "output_tokens = [vocab_transform['en'].lookup_token(tok) for tok in output_ids[0]]\n",
    "output_sentence = \" \".join([tok for tok in output_tokens if tok not in [\"<bos>\", \"<eos>\", \"<pad>\"]])\n"
   ],
   "id": "8cd0e6ea39991268",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T04:34:41.094987Z",
     "start_time": "2025-04-10T04:34:41.091987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"\\nSource (DE):     {src_text}\")\n",
    "print(f\"Generated (EN):  {output_sentence}\")\n",
    "print(f\"Reference (EN):  {en_sentence}\")"
   ],
   "id": "dbefdb39c4f40589",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source (DE):     eine gruppe von männern lädt baumwolle auf einen lastwagen\n",
      "Generated (EN):  a group of men loading vons shopping bags on a truck .\n",
      "Reference (EN):  A group of men are loading cotton onto a truck\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T04:34:41.111572Z",
     "start_time": "2025-04-10T04:34:41.106681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bleu = sacrebleu.corpus_bleu([output_sentence], [[en_sentence]])\n",
    "print(f\"\\n✅ Corpus BLEU score: {bleu.score:.2f}\")"
   ],
   "id": "26d53393946bd860",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Corpus BLEU score: 16.59\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
